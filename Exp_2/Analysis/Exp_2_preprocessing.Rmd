---
title: "Pilot Analysis"
author: "Will Clapp"
date: "8/25/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(psych)
library(lme4)
```


Load data
```{r}
df.data <- read.csv("/Users/willclapp/Desktop/QP1/QP1_Experiments/Exp_2/Analysis/Exp_2_raw.csv")
```


Remove any trials that took longer than 4 seconds (including the 1-second ISI) or less than one second (which means they somehow got around the ISI. Not sure how people did this.)
This knocks us down from the original 69460 obs. to 60539 (8921 observations lost)
```{r}
df.data <- df.data %>% 
  filter(trial_time.1 < 4) %>%
  filter(trial_time.1 > 1)

```

Now we need to see if removing those observations means that certain people don't actually sufficient exposure to critical ambiguous items
```{r}
df.no_crit_amb <- df.data %>% 
  filter(type == "crit_amb" | type == "crit_unamb") %>% 
  group_by(participant_id, type) %>% 
  tally() %>% 
  filter(n < 10)
```


We probably shouldn't include people who missed at least 10 critical words from either the critical ambiguous or unambiguous set. It looks like this removes three further people. (61701 obs. to 60027 obs.)
```{r}
df.data <-  df.data %>% 
  filter(!(participant_id %in% df.no_crit_amb$participant_id))

```


Create a binary variable where a "yes" response is 1 and "no" is 0
```{r}
df.data <- df.data %>% 
  mutate(response_01 = ifelse(response=="yes", 1, 0))

```

Add a centered version of step that's sensitive to the midpoint of each continuum identified by the norming experiments

```{r}
df.data <- df.data %>% 
  mutate("step_dist" = (step - normed_midpoint)) %>% 
  mutate("side_length" = ifelse((step>normed_midpoint), (30-normed_midpoint), (normed_midpoint))) %>% 
  mutate("norm_step" = step_dist/side_length)

```

And a mean-centered version of regular step too
```{r}
# mean center it
df.data = df.data %>% 
  mutate("c_step" = scale(step, scale=FALSE))
```

Add column for accuracy on exposure
```{r}
df.data <- df.data %>% 
  mutate("accurate" = ifelse(((phase=="exposure") & (((response=="yes") & (correct=="yes")) | ((response=="no") & (correct=="no")))), 1, 0))

df.data <- df.data %>% 
  mutate("accurate" = ifelse((phase=="exposure"), accurate, NA))

df.accuracy = df.data %>% 
  filter(phase == "exposure") %>% 
  group_by(participant_id, time_in_minutes) %>% 
  summarize(accuracy_exposure = mean(accurate))

df.data = left_join(df.data, df.accuracy, by="participant_id")

```



Add another column representing accuracy on test. This will be defined as the percentage of unambiguous stims that were categorized correctly.
```{r}
df.0_30 <- df.data %>% 
  filter(phase == "test" & (step == 0 | step == 30))

df.0_30 <- df.0_30 %>% 
  mutate("accurate_test" = ifelse(((response=="yes" & step == 30) | (response == "no" & step == 0)), 1, 0))


df.accuracy_test = df.0_30 %>% 
  group_by(participant_id) %>% 
  summarise(accuracy_test = mean(accurate_test))

df.data = left_join(df.data, df.accuracy_test, by="participant_id")

```




create two DFs, where `df.guessers` consists of removed participants and `df.listeners` consists of data to be analyzed. Current cutoff is 95% on exposure and 90% on test
```{r}
df.guessers <- df.data %>% 
  filter(accuracy_exposure < 0.95 | accuracy_test < 0.9)

df.listeners <- df.data %>% 
  filter(accuracy_exposure >= 0.95 & accuracy_test >= 0.9)
```


Time to figure out how many people are in each group. Looks like we kept 89 and filtered 56. yeesh
```{r}
df.groups_listeners <- df.listeners %>% 
  distinct(participant_id, .keep_all = TRUE)

df.macro_groups_listeners <- df.groups_listeners %>% 
  group_by(group) %>% 
  tally()

df.groups_guessers <- df.guessers %>% 
  distinct(participant_id, .keep_all = TRUE)

df.macro_groups_guessers <- df.groups_guessers %>% 
  group_by(group) %>% 
  tally()

```


Add a column to the main df.data file indicating whether or not a participant was excluded
```{r}
df.data <- df.data %>% 
  mutate("included" = ifelse((participant_id %in% df.groups_guessers$participant_id), 0, 1))

```






Visualize the critical ambiguous words to see how often people categorized them as expected. This looks pretty good. 
```{r}
df.listeners %>%
  filter(type == "crit_amb" & group != "control") %>% 
  ggplot(aes(x = accurate)) +
  geom_bar() +
  facet_wrap(~audio)

df.exposure = df.listeners %>% 
  filter(phase=="exposure")


mean(df.exposure$accuracy_exposure)

df.bcrit = df.exposure %>% 
  filter(group=="B_group" & type == "crit_amb")

mean(df.bcrit$accuracy_exposure)

df.pcrit = df.exposure %>% 
  filter(group=="P_group" & type == "crit_amb")

mean(df.pcrit$accuracy_exposure)

```


Then make a DF that consists only of the test phase and only included participants
```{r}
df.listeners_test <-df.listeners %>% 
  filter(phase == "test")

mean(df.listeners$time_in_minutes.x)

```



That should do it for pre-processing.

```{r}
# write.csv(df.listeners_test, "/Users/willclapp/Desktop/QP1/QP1_Experiments/Exp_2/Analysis/Exp_2_preprocessed.csv")

```



__________________________________________________________________________________________________
__________________________________________________________________________________________________
__________________________________________________________________________________________________
__________________________________________________________________________________________________
__________________________________________________________________________________________________



Okay, now getting stats for the write up. 




```{r}
df.all_data <- read.csv("/Users/willclapp/Desktop/QP1/QP1_Experiments/Exp_2/Analysis/Exp_2_raw.csv")

69460/460

df.exposure <- df.all_data %>% 
  filter(phase == "exposure")

df.test <- df.all_data %>% 
  filter(phase == "test")

mean(df.test$normed_midpoint)

mean(df.exposure$accuracy_exposure)

```



```{r}
df.one_trial <-  df.all_data %>% 
  filter(audio == "Violin")

mean(df.one_trial$time_in_minutes)
# = 25.6

#compensation = 4.76

(60*4.76)/26.88




```

```{r}
mean(df.one_trial$age)


summary(df.one_trial$age)


```








